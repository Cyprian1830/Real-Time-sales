# ğŸ“Š Real-Time Sales & Inventory Analytics with Azure
### Kappa / Lambda Hybrid Architecture

---

## ğŸ‘¥ Autorzy
- **BartÅ‚omiej Å»urek**
- **Cyprian Szot**

---

## ğŸ“ Opis projektu

Celem projektu jest budowa systemu **real-time analytics** do przetwarzania zdarzeÅ„ sprzedaÅ¼owych z wykorzystaniem usÅ‚ug **Microsoft Azure**.  
System obsÅ‚uguje zarÃ³wno **strumieniowe przetwarzanie danych (near real-time)**, jak i **warstwÄ™ batch**, zgodnie z podejÅ›ciem **Kappa / Lambda Hybrid**.

Zdarzenia sprzedaÅ¼y sÄ… generowane w czasie rzeczywistym, przetwarzane w **Azure Databricks**, a wyniki zapisywane do **Delta Lake (Azure Data Lake Storage Gen2)** oraz **Azure Database for PostgreSQL** w celu dalszej analizy SQL.

---

## ğŸ— Architektura i przepÅ‚yw danych

**PrzepÅ‚yw danych:**

1. Generator zdarzeÅ„ sprzedaÅ¼y (Python)
2. **Azure Event Hubs** â€“ ingest danych
3. **Azure Databricks (Structured Streaming)**
   - Bronze â€“ surowe dane
   - Silver â€“ agregacje czasowe
   - Gold â€“ dane analityczne i alerty stock-out
4. **Azure Data Lake Storage Gen2 (Delta Lake)**
5. **Azure Database for PostgreSQL** â€“ warstwa analityczna SQL
6. **Azure Monitor & Log Analytics** â€“ monitoring i alerty

---

## ğŸ”„ Warstwy danych

### ğŸŸ¤ Bronze Layer
- Surowe zdarzenia z Azure Event Hubs  
- Dane przechowywane w formacie **Delta Lake**

### âšª Silver Layer
- Agregacje czasowe (windowed aggregations)
- Metryki:
  - `total_qty`
  - `total_revenue`
- ObsÅ‚uga pÃ³l `eventTime`

### ğŸŸ¡ Gold Layer
- **Daily batch analytics**
- **Stock-out alerts**
- Dane gotowe do raportowania i zapytaÅ„ SQL

---

## ğŸš¨ Stock-out Detection

Stock-out definiowany jest jako:
> brak sprzedaÅ¼y danego produktu w danym sklepie w okreÅ›lonym oknie czasowym.

**Mechanizm detekcji:**
- agregacja strumieniowa danych sprzedaÅ¼owych
- filtracja zerowej sprzedaÅ¼y
- zapis alertÃ³w do warstwy Gold (`gold_stockout`)

---

## ğŸ—ƒ Warstwa Batch

Raz dziennie wykonywana jest agregacja batch obejmujÄ…ca:
- sprzedaÅ¼ dziennÄ…
- sumÄ™ sprzedanych iloÅ›ci
- sumÄ™ przychodÃ³w

**Wyniki zapisywane sÄ… do:**
- Delta Lake (`gold_daily_sales`)
- Azure Database for PostgreSQL (`gold_daily_sales`)

---

## ğŸ“ˆ Monitoring & Observability

System wykorzystuje **Azure Monitor** oraz **Log Analytics** do monitorowania kluczowych komponentÃ³w:

### Monitorowane usÅ‚ugi:
- **Azure Event Hubs**
  - liczba zdarzeÅ„
  - brak ingestu danych
- **Azure Databricks**
  - status jobÃ³w
  - bÅ‚Ä™dy przetwarzania
- **Azure Storage**
  - dostÄ™pnoÅ›Ä‡
  - bÅ‚Ä™dy zapisu
- **PostgreSQL**
  - poÅ‚Ä…czenia
  - dostÄ™pnoÅ›Ä‡ bazy

### Skonfigurowane alerty:
- brak danych wejÅ›ciowych
- problemy operacyjne workspace
- przekroczenie limitÃ³w ingestu w Log Analytics

---

## ğŸ§  Technologie

- Azure Event Hubs  
- Azure Databricks (PySpark, Structured Streaming)  
- Delta Lake  
- Azure Data Lake Storage Gen2  
- Azure Database for PostgreSQL  
- Azure Monitor & Log Analytics  
- Terraform  
- GitHub Actions  

---
## Uruchomienie projektu

W pierszej kolejnoÅ›ci naleÅ¼y utworzyÄ‡ zasoby wÅ‚Ä…czajÄ…c terminal z poziomu katalogu w ktÃ³rym zawarte sÄ… kody do utworzenia zasobÃ³w. 
NastÄ™pnie uruchamiamy komenda:
- terrafrom init
- terraform plan (PodajÄ…c hasÅ‚o do PostgreSQL np. PgSales2025)
- terraform apply

NastÄ™pnym krokiem jest utworzenie kontenerÃ³w w storage(robimy to rÄ™cznie w Azure):
- bronze
- silver
- gold
- checkpoints

NastÄ™pnym krokiem jest uruchomienie pliki z Pythona.
W tym celu odpalamy konsole i nastÄ™pnie:
- $env:EVENTHUB_CONNECTION_STRING="Endpoint=sb://..." (tu wklejamy connection string z Azura)
- $env:EVENTHUB_NAME="sales-events"
- python C:\Users\cypri\OneDrive\Pulpit\chmury\skrypt_projekt\skrypt.py (uruchomienie kodu)

Od tej pory postÄ™pujemy zgodnie z kodem zawartym w Databricks.



